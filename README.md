# PMLM
PMLM: Enhanced Breast Nodule Diagnosis through Parallel Multimodal Representations and Large Language Models

(PMLM, Personalized Multimodal Language Model).

### Notes

* "pre-training.py": Model for ptr-training. 
* "PMLM_train_and_test.py": Model for generating a breast ultrasound report description and diagnosis.
* "dataprocess.py": File used for processing hospital data.

### Requirements:

* transformers>=4.41.2
* datasets>=2.16.0
* accelerate>=0.30.1
* peft>=0.11.1
* trl>=0.8.6
* gradio>=4.0.0
* scipy
* einops
* sentencepiece
* tiktoken
* protobuf
* uvicorn
* pydantic
* fastapi
* sse-starlette
* fire
* packaging
* pyyaml

### Pre-training
* Framework
![image](/Figure/Arch.png)
Note: For breast nodule diagnosis, we aim to fully mine the information of both modalities, image, and text, and perform multimodal feature fusion through three parallel modules to improve the accuracy of diagnosis. We propose a deep learning model architecture, PMLM, containing three parallel modules. The proposed method consists of three main modules: (1) unimodal learner for image, (2) unimodal learner for text, and (3) multimodal learner.

### Case analysis
* case analysis
![image](/Figure/case1.png)

Note: This study aimed to assess the risk of breast lesions being benign or malignant by generating detailed textual descriptions and inferring a diagnosis. The reports generated by the physician and the model were highly consistent in structure and descriptive detail, with both being able to record important features such as the location and size of the mass when describing malignant tumours. While the model could capture basic imaging features, the lack of pathology data limited its diagnostic reasoning for complex lesions. For example, it could not accurately reflect clinical details such as cortical thickening of the lymph nodes and prominent vascular signals. This case study demonstrates the effectiveness and limitations of the model in generating reports for malignant and benign tumours. Future research should focus on integrating multimodal data to improve diagnostic accuracy and the clinical interpretability of reports.	


### Citation
If this work is helpful for you, please cite our paper as follows:

```

```
[DOI]()

### Contact details
If you have any questions please contact us. 

Email: mail@patrickpang.net  (Patrick Cheong-Iao Pang); taotanjs@gmail.com (Tao Tan); P2212871@mpu.edu.mo (Dashun Zheng)

